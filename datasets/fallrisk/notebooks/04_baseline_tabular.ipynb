{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 \u00b7 Baseline tabular models\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ORG/Fallrisk-gait/blob/main/datasets/fallrisk/notebooks/04_baseline_tabular.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train pure-Python baselines on the synthetic table: logistic regression and gradient boosting stumps for both the binary high-risk label and the 3-class policy. Report AUROC, macro-F1, calibration curves, and demographic slice metrics (sex and age bands)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def locate_repo_root(max_depth: int = 6) -> Path:\n",
        "    here = Path.cwd()\n",
        "    for _ in range(max_depth):\n",
        "        if (here / 'datasets').exists() and (here / 'data').exists():\n",
        "            return here\n",
        "        if here.parent == here:\n",
        "            break\n",
        "        here = here.parent\n",
        "    return Path.cwd()\n",
        "\n",
        "ROOT = locate_repo_root()\n",
        "data_path = ROOT / 'datasets' / 'fallrisk' / 'fallrisk_tabular_v1.csv'\n",
        "with data_path.open() as f:\n",
        "    rows = list(csv.DictReader(f))\n",
        "print(f'Loaded {len(rows)} synthetic rows')\n",
        "random.seed(8)\n",
        "random.shuffle(rows)\n",
        "rows = rows[:15000]\n",
        "split = int(0.8 * len(rows))\n",
        "train_rows = rows[:split]\n",
        "test_rows = rows[split:]\n",
        "FEATURES = ['age_years','bmi','systolic_bp','gait_speed_m_s','postural_sway_cm','medication_count',\n",
        "             'chronic_conditions','past_falls_6mo','dual_task_cost_percent','fear_of_falling_score',\n",
        "             'muscle_strength_score','reaction_time_ms']\n",
        "CLASSES = ['low','moderate','high']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_float(row, key):\n",
        "    return float(row[key])\n",
        "\n",
        "class StandardScaler:\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        self.stats = {feature: (0.0, 1.0) for feature in features}\n",
        "\n",
        "    def fit(self, data):\n",
        "        for feat in self.features:\n",
        "            vals = [to_float(r, feat) for r in data]\n",
        "            mean_val = sum(vals) / len(vals)\n",
        "            var = sum((v - mean_val) ** 2 for v in vals) / len(vals)\n",
        "            std = math.sqrt(var) if var > 0 else 1.0\n",
        "            self.stats[feat] = (mean_val, std)\n",
        "\n",
        "    def transform(self, row):\n",
        "        return [(to_float(row, feat) - self.stats[feat][0]) / (self.stats[feat][1] or 1.0) for feat in self.features]\n",
        "\n",
        "def logistic(x):\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "class BinaryLogisticRegression:\n",
        "    def __init__(self, lr=0.12, epochs=120):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.weights = [0.0] * (len(X[0]) + 1)\n",
        "        n = len(X)\n",
        "        for epoch in range(self.epochs):\n",
        "            grad0 = 0.0\n",
        "            grad = [0.0] * len(X[0])\n",
        "            for xi, yi in zip(X, y):\n",
        "                z = self.weights[0] + sum(w * v for w, v in zip(self.weights[1:], xi))\n",
        "                p = logistic(z)\n",
        "                diff = p - yi\n",
        "                grad0 += diff\n",
        "                for j in range(len(xi)):\n",
        "                    grad[j] += diff * xi[j]\n",
        "            step = self.lr / (1 + 0.01 * epoch)\n",
        "            self.weights[0] -= step * grad0 / n\n",
        "            for j in range(len(grad)):\n",
        "                self.weights[j + 1] -= step * grad[j] / n\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = []\n",
        "        for xi in X:\n",
        "            z = self.weights[0] + sum(w * v for w, v in zip(self.weights[1:], xi))\n",
        "            probs.append(logistic(z))\n",
        "        return probs\n",
        "\n",
        "class GradientBoostingStumps:\n",
        "    def __init__(self, n_estimators=18, learning_rate=0.18):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.trees = []\n",
        "        self.base_logit = 0.0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        base_rate = sum(y) / len(y)\n",
        "        base_rate = min(max(base_rate, 1e-6), 1 - 1e-6)\n",
        "        self.base_logit = math.log(base_rate / (1 - base_rate))\n",
        "        preds = [self.base_logit] * len(X)\n",
        "        for _ in range(self.n_estimators):\n",
        "            probs = [logistic(val) for val in preds]\n",
        "            residuals = [yt - pr for yt, pr in zip(y, probs)]\n",
        "            tree = self._fit_stump(X, residuals)\n",
        "            self.trees.append(tree)\n",
        "            for i, xi in enumerate(X):\n",
        "                update = tree['left_value'] if xi[tree['feature']] <= tree['threshold'] else tree['right_value']\n",
        "                preds[i] += self.learning_rate * update\n",
        "\n",
        "    def _fit_stump(self, X, residuals):\n",
        "        best = {'loss': float('inf')}\n",
        "        total_sum = sum(residuals)\n",
        "        total_sq = sum(r * r for r in residuals)\n",
        "        total_count = len(residuals)\n",
        "        n_features = len(X[0])\n",
        "        for feat in range(n_features):\n",
        "            pairs = sorted(((xi[feat], res) for xi, res in zip(X, residuals)), key=lambda x: x[0])\n",
        "            prefix_sum = 0.0\n",
        "            prefix_sq = 0.0\n",
        "            prefix_count = 0\n",
        "            for i in range(len(pairs) - 1):\n",
        "                val, res = pairs[i]\n",
        "                prefix_sum += res\n",
        "                prefix_sq += res * res\n",
        "                prefix_count += 1\n",
        "                if pairs[i + 1][0] == val:\n",
        "                    continue\n",
        "                left_count = prefix_count\n",
        "                right_count = total_count - prefix_count\n",
        "                if left_count == 0 or right_count == 0:\n",
        "                    continue\n",
        "                left_sum = prefix_sum\n",
        "                right_sum = total_sum - prefix_sum\n",
        "                left_sq = prefix_sq\n",
        "                right_sq = total_sq - prefix_sq\n",
        "                left_mean = left_sum / left_count\n",
        "                right_mean = right_sum / right_count\n",
        "                left_loss = left_sq - left_sum * left_sum / left_count\n",
        "                right_loss = right_sq - right_sum * right_sum / right_count\n",
        "                loss = left_loss + right_loss\n",
        "                if loss < best['loss']:\n",
        "                    threshold = (val + pairs[i + 1][0]) / 2.0\n",
        "                    best = {\n",
        "                        'feature': feat,\n",
        "                        'threshold': threshold,\n",
        "                        'left_value': left_mean,\n",
        "                        'right_value': right_mean,\n",
        "                        'loss': loss\n",
        "                    }\n",
        "        if best['loss'] == float('inf'):\n",
        "            best = {'feature': 0, 'threshold': 0.0, 'left_value': 0.0, 'right_value': 0.0, 'loss': 0.0}\n",
        "        return best\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = []\n",
        "        for xi in X:\n",
        "            logit_val = self.base_logit\n",
        "            for tree in self.trees:\n",
        "                update = tree['left_value'] if xi[tree['feature']] <= tree['threshold'] else tree['right_value']\n",
        "                logit_val += self.learning_rate * update\n",
        "            probs.append(logistic(logit_val))\n",
        "        return probs\n",
        "\n",
        "def auc_score(probs, labels):\n",
        "    pairs = sorted(zip(probs, labels), key=lambda x: x[0])\n",
        "    pos = sum(labels)\n",
        "    neg = len(labels) - pos\n",
        "    if pos == 0 or neg == 0:\n",
        "        return None\n",
        "    rank = 0.0\n",
        "    cum_pos = 0\n",
        "    for idx, (score, label) in enumerate(pairs, start=1):\n",
        "        if label == 1:\n",
        "            rank += idx\n",
        "            cum_pos += 1\n",
        "    return (rank - cum_pos * (cum_pos + 1) / 2) / (pos * neg)\n",
        "\n",
        "def macro_f1(labels, preds):\n",
        "    cm = Counter()\n",
        "    for yt, yp in zip(labels, preds):\n",
        "        cm[(yt, yp)] += 1\n",
        "    def f1_for(target):\n",
        "        tp = cm[(target, target)]\n",
        "        fp = cm[(1 - target, target)]\n",
        "        fn = cm[(target, 1 - target)]\n",
        "        precision = tp / (tp + fp) if tp + fp else 0.0\n",
        "        recall = tp / (tp + fn) if tp + fn else 0.0\n",
        "        return 0.0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "    return 0.5 * (f1_for(0) + f1_for(1))\n",
        "\n",
        "def calibration_curve(probs, labels, bins=10):\n",
        "    counts = [0] * bins\n",
        "    prob_sum = [0.0] * bins\n",
        "    label_sum = [0.0] * bins\n",
        "    for p, y in zip(probs, labels):\n",
        "        idx = min(bins - 1, int(p * bins))\n",
        "        counts[idx] += 1\n",
        "        prob_sum[idx] += p\n",
        "        label_sum[idx] += y\n",
        "    curve = []\n",
        "    for i in range(bins):\n",
        "        if counts[i] == 0:\n",
        "            curve.append({'bin': i, 'count': 0, 'pred': None, 'actual': None})\n",
        "        else:\n",
        "            curve.append({'bin': i, 'count': counts[i], 'pred': prob_sum[i] / counts[i], 'actual': label_sum[i] / counts[i]})\n",
        "    return curve\n",
        "\n",
        "def slice_macro_f1(rows, probs, labels, threshold=0.5):\n",
        "    grouped = defaultdict(list)\n",
        "    for row, prob, label in zip(rows, probs, labels):\n",
        "        age = float(row['age_years'])\n",
        "        if age < 70:\n",
        "            age_group = '<70'\n",
        "        elif age < 80:\n",
        "            age_group = '70-79'\n",
        "        else:\n",
        "            age_group = '80+'\n",
        "        grouped[('sex', row['sex'])].append((prob, label))\n",
        "        grouped[('age_group', age_group)].append((prob, label))\n",
        "    metrics = {}\n",
        "    for key, items in grouped.items():\n",
        "        preds = [1 if p >= threshold else 0 for p, _ in items]\n",
        "        true = [y for _, y in items]\n",
        "        metrics[key] = {\n",
        "            'count': len(items),\n",
        "            'macro_f1': round(macro_f1(true, preds), 3)\n",
        "        }\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler(FEATURES)\n",
        "scaler.fit(train_rows)\n",
        "X_train = [scaler.transform(r) for r in train_rows]\n",
        "y_train = [int(r['label_high_fall_risk']) for r in train_rows]\n",
        "X_test = [scaler.transform(r) for r in test_rows]\n",
        "y_test = [int(r['label_high_fall_risk']) for r in test_rows]\n",
        "\n",
        "logreg = BinaryLogisticRegression(lr=0.15, epochs=140)\n",
        "logreg.fit(X_train, y_train)\n",
        "log_probs = logreg.predict_proba(X_test)\n",
        "log_preds = [1 if p >= 0.5 else 0 for p in log_probs]\n",
        "log_auc = auc_score(log_probs, y_test)\n",
        "log_macro_f1 = macro_f1(y_test, log_preds)\n",
        "print(f'Logistic regression \u2192 AUROC: {log_auc:.3f}, macro-F1: {log_macro_f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb = GradientBoostingStumps()\n",
        "gb.fit(X_train, y_train)\n",
        "gb_probs = gb.predict_proba(X_test)\n",
        "gb_preds = [1 if p >= 0.5 else 0 for p in gb_probs]\n",
        "gb_auc = auc_score(gb_probs, y_test)\n",
        "gb_macro_f1 = macro_f1(y_test, gb_preds)\n",
        "print(f'Gradient boosting stumps \u2192 AUROC: {gb_auc:.3f}, macro-F1: {gb_macro_f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calibration = calibration_curve(log_probs, y_test)\n",
        "for entry in calibration[:3]:\n",
        "    print('Calibration bin', entry)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slices = slice_macro_f1(test_rows, log_probs, y_test)\n",
        "for key, info in slices.items():\n",
        "    print('Slice', key, '\u2192', info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneVsRestLogistic:\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, X, labels):\n",
        "        for cls in self.classes:\n",
        "            binary = [1 if label == cls else 0 for label in labels]\n",
        "            model = BinaryLogisticRegression(lr=0.12, epochs=120)\n",
        "            model.fit(X, binary)\n",
        "            self.models[cls] = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = []\n",
        "        for xi in X:\n",
        "            scores = {}\n",
        "            for cls, model in self.models.items():\n",
        "                score = model.predict_proba([xi])[0]\n",
        "                scores[cls] = score\n",
        "            total = sum(scores.values())\n",
        "            if total == 0:\n",
        "                probs = {cls: 1.0 / len(self.classes) for cls in self.classes}\n",
        "            else:\n",
        "                probs = {cls: val / total for cls, val in scores.items()}\n",
        "            outputs.append(probs)\n",
        "        return outputs\n",
        "\n",
        "class OneVsRestGB:\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, X, labels):\n",
        "        for cls in self.classes:\n",
        "            binary = [1 if label == cls else 0 for label in labels]\n",
        "            model = GradientBoostingStumps()\n",
        "            model.fit(X, binary)\n",
        "            self.models[cls] = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = []\n",
        "        for xi in X:\n",
        "            scores = {}\n",
        "            for cls, model in self.models.items():\n",
        "                score = model.predict_proba([xi])[0]\n",
        "                scores[cls] = score\n",
        "            total = sum(scores.values())\n",
        "            if total == 0:\n",
        "                probs = {cls: 1.0 / len(self.classes) for cls in self.classes}\n",
        "            else:\n",
        "                probs = {cls: val / total for cls, val in scores.items()}\n",
        "            outputs.append(probs)\n",
        "        return outputs\n",
        "\n",
        "def macro_f1_multiclass(true_labels, pred_labels):\n",
        "    scores = []\n",
        "    for cls in CLASSES:\n",
        "        tp = sum(1 for yt, yp in zip(true_labels, pred_labels) if yt == cls and yp == cls)\n",
        "        fp = sum(1 for yt, yp in zip(true_labels, pred_labels) if yt != cls and yp == cls)\n",
        "        fn = sum(1 for yt, yp in zip(true_labels, pred_labels) if yt == cls and yp != cls)\n",
        "        precision = tp / (tp + fp) if tp + fp else 0.0\n",
        "        recall = tp / (tp + fn) if tp + fn else 0.0\n",
        "        f1 = 0.0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "        scores.append(f1)\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def multiclass_auc(true_labels, prob_dicts):\n",
        "    aucs = []\n",
        "    for cls in CLASSES:\n",
        "        binary_true = [1 if label == cls else 0 for label in true_labels]\n",
        "        binary_scores = [prob[cls] for prob in prob_dicts]\n",
        "        auc = auc_score(binary_scores, binary_true)\n",
        "        if auc is not None:\n",
        "            aucs.append(auc)\n",
        "    return sum(aucs) / len(aucs) if aucs else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels = [row['label_risk_level'] for row in train_rows]\n",
        "test_labels = [row['label_risk_level'] for row in test_rows]\n",
        "ovr_logreg = OneVsRestLogistic(CLASSES)\n",
        "ovr_logreg.fit(X_train, train_labels)\n",
        "log_multi_probs = ovr_logreg.predict(X_test)\n",
        "log_multi_preds = [max(prob.items(), key=lambda x: x[1])[0] for prob in log_multi_probs]\n",
        "log_multi_f1 = macro_f1_multiclass(test_labels, log_multi_preds)\n",
        "log_multi_auc = multiclass_auc(test_labels, log_multi_probs)\n",
        "print(f'Logistic OvR (3-class) \u2192 AUROC: {log_multi_auc:.3f}, macro-F1: {log_multi_f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ovr_gb = OneVsRestGB(CLASSES)\n",
        "ovr_gb.fit(X_train, train_labels)\n",
        "gb_multi_probs = ovr_gb.predict(X_test)\n",
        "gb_multi_preds = [max(prob.items(), key=lambda x: x[1])[0] for prob in gb_multi_probs]\n",
        "gb_multi_f1 = macro_f1_multiclass(test_labels, gb_multi_preds)\n",
        "gb_multi_auc = multiclass_auc(test_labels, gb_multi_probs)\n",
        "print(f'Gradient boosting OvR (3-class) \u2192 AUROC: {gb_multi_auc:.3f}, macro-F1: {gb_multi_f1:.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}