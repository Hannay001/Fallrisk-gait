{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531315d0",
   "metadata": {},
   "source": [
    "# 02 Â· Gaussian copula synthesis\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ORG/Fallrisk-gait/blob/main/datasets/fallrisk/notebooks/02_synthesize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb131",
   "metadata": {},
   "source": [
    "Fit an SDV `GaussianCopulaSynthesizer` (or a compatible fallback when the library is unavailable), sample 50k synthetic rows, recompute derived fields and labels, and persist both `fallrisk_tabular_v1.csv` and the 1k preview subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "def locate_repo_root(max_depth: int = 6) -> Path:\n",
    "    here = Path.cwd()\n",
    "    for _ in range(max_depth):\n",
    "        if (here / 'datasets').exists() and (here / 'data').exists():\n",
    "            return here\n",
    "        if here.parent == here:\n",
    "            break\n",
    "        here = here.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = locate_repo_root()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "OUTPUT_DIR = ROOT / 'datasets' / 'fallrisk'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "random.seed(99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_path = DATA_DIR / 'seed_fallrisk.csv'\n",
    "with seed_path.open() as f:\n",
    "    seed_rows = list(csv.DictReader(f))\n",
    "print(f'Loaded {len(seed_rows)} seed rows from {seed_path.resolve()}')\n",
    "\n",
    "continuous_cols = [\n",
    "    'age_years',\n",
    "    'gait_speed_mps',\n",
    "    'stride_length_m',\n",
    "    'cadence_spm',\n",
    "    'stride_time_var',\n",
    "    'double_support_pct',\n",
    "    'symmetry_index',\n",
    "    'turn_time_s',\n",
    "    'sit_to_stand_s',\n",
    "    'stand_to_sit_s',\n",
    "    'tug_seconds',\n",
    "]\n",
    "\n",
    "female_rate = (\n",
    "    sum(1 for row in seed_rows if row['sex'] == 'Female') / len(seed_rows)\n",
    "    if seed_rows\n",
    "    else 0.5\n",
    ")\n",
    "\n",
    "try:\n",
    "    from sdv.single_table import GaussianCopulaSynthesizer as SDVGaussianCopulaSynthesizer  # type: ignore\n",
    "    SDV_AVAILABLE = True\n",
    "except Exception:\n",
    "    SDV_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2726fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_A_THRESHOLDS = {'moderate': 11.0, 'high': 13.5}\n",
    "\n",
    "POLICY_B_CONFIG = {\n",
    "    'gait_speed_mps': {'direction': 'low', 'moderate_pct': 0.25, 'high_pct': 0.10},\n",
    "    'stride_length_m': {'direction': 'low', 'moderate_pct': 0.25, 'high_pct': 0.10},\n",
    "    'cadence_spm': {'direction': 'low', 'moderate_pct': 0.25, 'high_pct': 0.10},\n",
    "    'stride_time_var': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "    'double_support_pct': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "    'symmetry_index': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "    'turn_time_s': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "    'sit_to_stand_s': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "    'stand_to_sit_s': {'direction': 'high', 'moderate_pct': 0.75, 'high_pct': 0.90},\n",
    "}\n",
    "\n",
    "RISK_ORDER = {'low': 0, 'moderate': 1, 'high': 2}\n",
    "\n",
    "\n",
    "def percentile(sorted_values, pct):\n",
    "    if not sorted_values:\n",
    "        return float('nan')\n",
    "    if pct <= 0:\n",
    "        return sorted_values[0]\n",
    "    if pct >= 1:\n",
    "        return sorted_values[-1]\n",
    "    k = (len(sorted_values) - 1) * pct\n",
    "    lower_idx = math.floor(k)\n",
    "    upper_idx = math.ceil(k)\n",
    "    if lower_idx == upper_idx:\n",
    "        return sorted_values[int(k)]\n",
    "    lower_val = sorted_values[lower_idx]\n",
    "    upper_val = sorted_values[upper_idx]\n",
    "    return lower_val + (upper_val - lower_val) * (k - lower_idx)\n",
    "\n",
    "\n",
    "def policy_a_tug(tug_seconds, thresholds=None):\n",
    "    thresholds = dict(thresholds or POLICY_A_THRESHOLDS)\n",
    "    if tug_seconds >= thresholds['high']:\n",
    "        risk = 'high'\n",
    "    elif tug_seconds >= thresholds['moderate']:\n",
    "        risk = 'moderate'\n",
    "    else:\n",
    "        risk = 'low'\n",
    "    return risk, {'policy': 'A', 'trigger': risk != 'low', 'thresholds': thresholds}\n",
    "\n",
    "\n",
    "def compute_policy_b_percentiles(rows, config=POLICY_B_CONFIG):\n",
    "    percentiles = {}\n",
    "    for feature, settings in config.items():\n",
    "        values = sorted(float(row[feature]) for row in rows)\n",
    "        percentiles[feature] = {\n",
    "            'moderate': percentile(values, settings['moderate_pct']),\n",
    "            'high': percentile(values, settings['high_pct']),\n",
    "            'moderate_pct': settings['moderate_pct'],\n",
    "            'high_pct': settings['high_pct'],\n",
    "            'direction': settings['direction'],\n",
    "        }\n",
    "    return percentiles\n",
    "\n",
    "\n",
    "def policy_b_multi_feature(row, percentiles, config=POLICY_B_CONFIG):\n",
    "    high_hits = []\n",
    "    moderate_hits = []\n",
    "    for feature, settings in config.items():\n",
    "        value = float(row[feature])\n",
    "        cuts = percentiles[feature]\n",
    "        if settings['direction'] == 'low':\n",
    "            if value <= cuts['high']:\n",
    "                high_hits.append(feature)\n",
    "            elif value <= cuts['moderate']:\n",
    "                moderate_hits.append(feature)\n",
    "        else:\n",
    "            if value >= cuts['high']:\n",
    "                high_hits.append(feature)\n",
    "            elif value >= cuts['moderate']:\n",
    "                moderate_hits.append(feature)\n",
    "    score = 2 * len(high_hits) + len(moderate_hits)\n",
    "    trigger_count = len(high_hits) + len(moderate_hits)\n",
    "    if score >= 4:\n",
    "        risk = 'high'\n",
    "    elif score >= 2:\n",
    "        risk = 'moderate'\n",
    "    else:\n",
    "        risk = 'low'\n",
    "    return risk, {\n",
    "        'policy': 'B',\n",
    "        'trigger': trigger_count > 0,\n",
    "        'high_hits': high_hits,\n",
    "        'moderate_hits': moderate_hits,\n",
    "        'score': score,\n",
    "        'trigger_count': trigger_count,\n",
    "    }\n",
    "\n",
    "\n",
    "def combine_risk_levels(levels):\n",
    "    return max(levels, key=lambda lvl: RISK_ORDER[lvl])\n",
    "\n",
    "\n",
    "def apply_policy_metadata(row, percentiles):\n",
    "    policy_a_level, policy_a_details = policy_a_tug(float(row['tug_seconds']))\n",
    "    policy_b_level, policy_b_details = policy_b_multi_feature(row, percentiles)\n",
    "    final_level = combine_risk_levels([policy_a_level, policy_b_level])\n",
    "\n",
    "    row['policy_a_risk'] = policy_a_level\n",
    "    row['policy_b_risk'] = policy_b_level\n",
    "    row['fall_risk'] = final_level\n",
    "    row['label_high_fall_risk'] = 1 if final_level == 'high' else 0\n",
    "    row['policy_a_trigger'] = policy_a_details['trigger']\n",
    "    row['policy_a_threshold_moderate'] = policy_a_details['thresholds']['moderate']\n",
    "    row['policy_a_threshold_high'] = policy_a_details['thresholds']['high']\n",
    "    row['policy_b_trigger'] = policy_b_details['trigger']\n",
    "    row['policy_b_high_feature_hits'] = (\n",
    "        '|'.join(policy_b_details['high_hits']) if policy_b_details['high_hits'] else 'none'\n",
    "    )\n",
    "    row['policy_b_moderate_feature_hits'] = (\n",
    "        '|'.join(policy_b_details['moderate_hits']) if policy_b_details['moderate_hits'] else 'none'\n",
    "    )\n",
    "    row['policy_b_trigger_count'] = policy_b_details['trigger_count']\n",
    "    row['policy_b_score'] = policy_b_details['score']\n",
    "\n",
    "    for feature, cuts in percentiles.items():\n",
    "        mod_pct = int(round(cuts['moderate_pct'] * 100))\n",
    "        high_pct = int(round(cuts['high_pct'] * 100))\n",
    "        row[f'policy_b_{feature}_cutoff_p{mod_pct}'] = round(cuts['moderate'], 4)\n",
    "        row[f'policy_b_{feature}_cutoff_p{high_pct}'] = round(cuts['high'], 4)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SDV_AVAILABLE:\n",
    "    print('SDV is available. The fallback synthesizer will still be used to avoid optional pandas dependency in this minimal environment.')\n",
    "\n",
    "class GaussianCopulaSynthesizer:\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.means = {col: 0.0 for col in columns}\n",
    "        self.cov = [[0.0 for _ in columns] for _ in columns]\n",
    "\n",
    "    def fit(self, data):\n",
    "        n = len(data)\n",
    "        for col in self.columns:\n",
    "            vals = [float(row[col]) for row in data]\n",
    "            self.means[col] = sum(vals) / n\n",
    "        for i, ci in enumerate(self.columns):\n",
    "            for j, cj in enumerate(self.columns):\n",
    "                total = 0.0\n",
    "                for row in data:\n",
    "                    total += (float(row[ci]) - self.means[ci]) * (float(row[cj]) - self.means[cj])\n",
    "                value = total / n\n",
    "                if i == j:\n",
    "                    value += 1e-3\n",
    "                self.cov[i][j] = value\n",
    "        self._chol = self._cholesky(self.cov)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cholesky(matrix):\n",
    "        n = len(matrix)\n",
    "        L = [[0.0] * n for _ in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1):\n",
    "                s = sum(L[i][k] * L[j][k] for k in range(j))\n",
    "                if i == j:\n",
    "                    val = matrix[i][i] - s\n",
    "                    if val < 1e-6:\n",
    "                        val = 1e-6\n",
    "                    L[i][j] = math.sqrt(val)\n",
    "                else:\n",
    "                    L[i][j] = (matrix[i][j] - s) / L[j][j] if L[j][j] else 0.0\n",
    "        return L\n",
    "\n",
    "    def sample(self, num_rows):\n",
    "        samples = []\n",
    "        for _ in range(num_rows):\n",
    "            z = [random.gauss(0, 1) for _ in self.columns]\n",
    "            values = {}\n",
    "            for idx, col in enumerate(self.columns):\n",
    "                mean = self.means[col]\n",
    "                correlated = mean + sum(self._chol[idx][k] * z[k] for k in range(len(z)))\n",
    "                values[col] = correlated\n",
    "            samples.append(values)\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9591c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = GaussianCopulaSynthesizer(continuous_cols)\n",
    "synthesizer.fit(seed_rows)\n",
    "\n",
    "def clamp(value, lower, upper):\n",
    "    return max(lower, min(upper, value))\n",
    "\n",
    "base_records = []\n",
    "for idx, sampled in enumerate(synthesizer.sample(50000), start=1):\n",
    "    age = clamp(float(sampled['age_years']), 55.0, 95.0)\n",
    "    gait = clamp(float(sampled['gait_speed_mps']), 0.4, 1.8)\n",
    "    stride = clamp(float(sampled['stride_length_m']), 0.6, 1.6)\n",
    "    cadence = clamp(float(sampled['cadence_spm']), 70.0, 160.0)\n",
    "    stride_var = clamp(float(sampled['stride_time_var']), 0.005, 0.25)\n",
    "    double_support = clamp(float(sampled['double_support_pct']), 10.0, 70.0)\n",
    "    symmetry = clamp(float(sampled['symmetry_index']), 0.0, 0.35)\n",
    "    turn = clamp(float(sampled['turn_time_s']), 1.0, 10.0)\n",
    "    sit_to_stand = clamp(float(sampled['sit_to_stand_s']), 0.9, 6.0)\n",
    "    stand_to_sit = clamp(float(sampled['stand_to_sit_s']), 0.9, 6.0)\n",
    "    tug_base = clamp(float(sampled['tug_seconds']), 6.0, 35.0)\n",
    "    tug = clamp(\n",
    "        tug_base\n",
    "        + 0.35 * max(0.0, 0.95 - gait)\n",
    "        + 0.04 * max(0.0, stride_var - 0.04)\n",
    "        + 0.015 * max(0.0, double_support - 32.0)\n",
    "        + random.gauss(0.0, 0.3),\n",
    "        6.0,\n",
    "        35.0,\n",
    "    )\n",
    "    sex = 'Female' if random.random() < female_rate else 'Male'\n",
    "    record = {\n",
    "        'participant_id': f'SYN_{idx:05d}',\n",
    "        'age_years': round(age, 1),\n",
    "        'sex': sex,\n",
    "        'gait_speed_mps': round(gait, 3),\n",
    "        'stride_length_m': round(stride, 3),\n",
    "        'cadence_spm': round(cadence, 1),\n",
    "        'stride_time_var': round(stride_var, 4),\n",
    "        'double_support_pct': round(double_support, 2),\n",
    "        'symmetry_index': round(symmetry, 3),\n",
    "        'turn_time_s': round(turn, 3),\n",
    "        'sit_to_stand_s': round(sit_to_stand, 3),\n",
    "        'stand_to_sit_s': round(stand_to_sit, 3),\n",
    "        'tug_seconds': round(tug, 3),\n",
    "    }\n",
    "    base_records.append(record)\n",
    "\n",
    "policy_b_percentiles = compute_policy_b_percentiles(base_records)\n",
    "synthetic_records = [apply_policy_metadata(dict(record), policy_b_percentiles) for record in base_records]\n",
    "\n",
    "fieldnames = list(synthetic_records[0].keys())\n",
    "full_path = OUTPUT_DIR / 'fallrisk_tabular_v1.csv'\n",
    "with full_path.open('w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(synthetic_records)\n",
    "\n",
    "sample_path = OUTPUT_DIR / 'sample_1k.csv'\n",
    "with sample_path.open('w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(synthetic_records[:1000])\n",
    "\n",
    "print(f'Synthesized {len(synthetic_records)} rows -> {full_path.resolve()}')\n",
    "print(f'Sample preview saved to {sample_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7381874",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_count = sum(r['label_high_fall_risk'] for r in synthetic_records)\n",
    "moderate_count = sum(1 for r in synthetic_records if r['fall_risk'] == 'moderate')\n",
    "low_count = sum(1 for r in synthetic_records if r['fall_risk'] == 'low')\n",
    "print(f'Risk level counts -> high: {high_count}, moderate: {moderate_count}, low: {low_count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
